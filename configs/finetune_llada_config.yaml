# sample config file for finetuning llada model
learning_rate: 2e-4
temperature: 0.0
alpha: 0.2
dataset: "yahma/alpaca-cleaned"
batch_size: 4
remask_ratio: 0.5