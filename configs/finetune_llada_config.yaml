# sample config file for finetuning llada model
learning_rate: 1e-5
temperature: 0.0
alpha: 0.2
dataset: "yahma/alpaca-cleaned"
batch_size: 4